{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6f9918e-bcc2-4788-a5ca-23f31ce83ecc",
   "metadata": {},
   "source": [
    "# Team- LevelUp, Submission Notebook\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates a process to scrape a website for links and content, generate questions based on the collected content, and save the results in JSON format. The workflow consists of 3 main parts:\n",
    "\n",
    "1. **Web Scraping**: Extracts links and content from a specified website.\n",
    "2. **Content Analysis**: Merges content, generates questions, and fetches titles for the links.\n",
    "3. **Data Validation**: Testing the output file on various parameters.\n",
    "\n",
    "![Overview Figure](overview.png)\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "Before running the notebook, ensure the following Python packages are installed:\n",
    "- `requests`\n",
    "- `beautifulsoup4`\n",
    "- `google-generativeai`\n",
    "- `json`\n",
    "- `subprocess`\n",
    "- `logging`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a677ed44-56ec-4bcd-b31f-2d287ee64078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachida/userdata/vscode/oosc_levelup/menv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "import json\n",
    "import google.generativeai as genai\n",
    "import subprocess\n",
    "from urllib.parse import urlparse\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd2e9ea-be0a-4ebe-8ed3-97b5a91f0c71",
   "metadata": {},
   "source": [
    "# Website Scraper Script\n",
    "\n",
    "This script is designed to scrape a given website, collect links, and extract relevant content and titles from those links. The process includes error handling and logging for better traceability.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Logging Configuration**: The script uses `logging.basicConfig` to set up logging with a debug level and a specific format that includes timestamps, log levels, and messages.\n",
    "\n",
    "- **Scrape Website Function**: The `scrape_website(url)` function:\n",
    "  - Logs the initiation of the scraping process.\n",
    "  - Attempts to fetch the main webpage content.\n",
    "  - Extracts all links from the main page.\n",
    "  - Extracts and appends content from each link, including headers and paragraphs.\n",
    "  - Handles exceptions, logging any errors encountered during the scraping.\n",
    "\n",
    "- **Content and Links**: The script gathers the main content and relevant links with titles, ensuring a clean and organized output.\n",
    "\n",
    "## Usage\n",
    "\n",
    "To use the script, simply call the `scrape_website(url)` function with the desired URL. The function returns the scraped content and a list of relevant links.\n",
    "\n",
    "```python\n",
    "content, links = scrape_website(\"https://example.com\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba13d47f-2e13-46af-91d3-26958d12bb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def scrape_website(url):\n",
    "    logging.info(f\"Scraping website: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Failed to fetch the main URL: {e}\")\n",
    "        return []\n",
    "\n",
    "    main_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = [link.get('href') for link in main_soup.find_all('a') if link.get('href')]\n",
    "    \n",
    "    logging.debug(f\"Found {len(links)} links on the main page\")\n",
    "\n",
    "    main_content = []\n",
    "    unique_links = set()\n",
    "    count = 0\n",
    "    relevant_links = []\n",
    "\n",
    "    for link in links:\n",
    "        # Construct full link if necessary\n",
    "        full_link = link if link.startswith('http') else f\"{url.rstrip('/')}/{link.lstrip('/')}\"\n",
    "\n",
    "        # Check if the link is unique and doesn't contain '#'\n",
    "        if '#' not in full_link and full_link not in unique_links:\n",
    "            unique_links.add(full_link)\n",
    "            logging.debug(f\"Processing link: {full_link}\")\n",
    "\n",
    "            try:\n",
    "                link_response = requests.get(full_link, timeout=5)\n",
    "                link_response.raise_for_status()  # Check for HTTP errors\n",
    "\n",
    "                soup = BeautifulSoup(link_response.text, 'html.parser')\n",
    "\n",
    "                title = soup.title.string.strip() if soup.title else \"No Title Found\"\n",
    "                relevant_links.append({\"link\": full_link, \"title\": title})\n",
    "                content = {\"link\": full_link}\n",
    "                txt = \"\"\n",
    "\n",
    "                # Iterate over all elements and add their text to the content list\n",
    "                for element in soup.find_all(['h1', 'h2', 'p']):\n",
    "                    txt += \" \" + element.get_text(strip=True)\n",
    "\n",
    "                content[\"text\"] = txt.strip() + '\\n'  # Ensure each content ends with a newline\n",
    "                main_content.append(content)\n",
    "                \n",
    "                count += 1\n",
    "                if count == 12:  # Stop after processing exactly 5 unique links\n",
    "                    break\n",
    "\n",
    "            except requests.RequestException as e:\n",
    "                logging.warning(f\"Failed to scrape {full_link}: {e}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Unexpected error when scraping {full_link}: {e}\")\n",
    "    \n",
    "    return main_content, relevant_links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5481f7e0-522b-475a-b4d3-b7645ca5a86d",
   "metadata": {},
   "source": [
    "# JSON String Parser\n",
    "\n",
    "This script provides a robust function to parse a JSON string that might be embedded within other text. It carefully trims the string to extract the JSON data and then attempts to parse it.\n",
    "\n",
    "## Features\n",
    "- **Error Handling**: The function includes comprehensive error handling to manage various scenarios:\n",
    "  - **`JSONDecodeError`**: Catches and logs errors if the JSON parsing fails.\n",
    "  - **`ValueError`**: Raises an error if no valid JSON data is found after trimming.\n",
    "  - **General Exception Handling**: Catches any other unexpected errors and logs them.\n",
    "\n",
    "- **Return Value**: If successful, the function returns the parsed JSON object. If an error occurs, it returns `None`.\n",
    "\n",
    "## Usage\n",
    "\n",
    "To use the function, pass the string containing the JSON data to `parse_json_string(json_string)`. The function will return the parsed JSON object or `None` if parsing fails.\n",
    "\n",
    "```python\n",
    "parsed_data = parse_json_string('Your string containing JSON data')\n",
    "if parsed_data:\n",
    "    print(parsed_data)\n",
    "else:\n",
    "    print(\"Failed to parse JSON data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93d8328b-2055-4a95-a844-8658ff54f5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_string(json_string):\n",
    "    try:\n",
    "        # Trim the string to get the JSON part between the first '[' and the last ']'\n",
    "        start_index = json_string.find('[')\n",
    "        end_index = json_string.rfind(']') + 1\n",
    "        trimmed_json_string = json_string[start_index:end_index].strip()\n",
    "\n",
    "        # Check if the trimmed string is empty\n",
    "        if not trimmed_json_string:\n",
    "            raise ValueError(\"No valid JSON data found in the string.\")\n",
    "\n",
    "        # Parse the JSON string\n",
    "        parsed_json = json.loads(trimmed_json_string)\n",
    "        return parsed_json\n",
    "    \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON decoding failed: {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Value error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3453efc5-13fe-4737-9fc9-af896fd222a9",
   "metadata": {},
   "source": [
    "# Website Content Scraping and Question Generation Pipeline\n",
    "\n",
    "The `process_website` function implements an end-to-end pipeline for extracting, processing, and validating web content, leveraging advanced AI capabilities for content analysis.\n",
    "\n",
    "## Key Functionalities\n",
    "\n",
    "1. **Web Scraping**:\n",
    "   - The function initiates the process by invoking the `scrape_website(url)` method, which performs HTTP requests to fetch the HTML content of the specified URL.\n",
    "   - It parses the HTML using `BeautifulSoup`, extracting relevant text and hyperlinks. The scraped data is serialized into a JSON structure and stored locally as `scraped_data.json`.\n",
    "\n",
    "2. **AI-Driven Content Analysis**:\n",
    "   - The function utilizes Google's Generative AI model (`gemini-1.5-flash`) to generate contextually relevant questions from the scraped textual content.\n",
    "   - A templated query is crafted to instruct the AI model to produce concise, general-purpose questions, each under 80 characters.\n",
    "   - The function aggregates these generated questions along with the corresponding relevant links until a threshold of 10 questions is met.\n",
    "\n",
    "3. **Data storage**:\n",
    "   - Post-processing, the output dataâ€”including the source URL, generated questions, and associated linksâ€”is encapsulated in a structured JSON format.\n",
    "   - The JSON data is then written to a file, uniquely named based on the domain of the processed URL, and saved under the `./outputs/` directory.\n",
    "\n",
    "\n",
    "## Usage Example\n",
    "\n",
    "Invoke the `process_website` function with a target URL:\n",
    "\n",
    "```python\n",
    "process_website('https://example.com')\n",
    "```\n",
    "The output file will be stored in `./outputs/example.com.json`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26fda5eb-d327-4afa-8b8d-9d47c085347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_website(url):\n",
    "    print(\"Scraping website...\")\n",
    "    scraped_data, relevent_links = scrape_website(url)\n",
    "    if scraped_data and relevent_links:\n",
    "        print(\"Website scraping completed...âœ… âœ…\")\n",
    "    else:\n",
    "        print(\"Error encountered while scraping...âŒâŒ\")\n",
    "        return\n",
    "    print(\"Generating Questions... Please wait...ðŸ•’ðŸ•’ðŸ•’\")\n",
    "    with open('scraped_data.json', 'w') as f:\n",
    "        json.dump(scraped_data, f, indent=4)\n",
    "\n",
    "    genai.configure(api_key=\"AIzaSyA9hLGJD5RjmB8OrAmwdL5zpSEzUiG3w1Y\")\n",
    "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "    query_template = \"from the following content generate 2 general questions, with length striclty less than 70 characters. (return a json array): \"\n",
    "\n",
    "    questions = []\n",
    "    rl = []\n",
    "    main_content = []\n",
    "\n",
    "    cnt = 0\n",
    "    ind = 0\n",
    "    for(content) in scraped_data:\n",
    "        ind = ind + 1\n",
    "        query = query_template + content[\"text\"]\n",
    "        response = model.generate_content(query)\n",
    "        q = parse_json_string(response.text)\n",
    "        if q:\n",
    "            cnt = cnt + len(q)\n",
    "            questions = questions + q\n",
    "            rl.append(relevent_links[ind-1])\n",
    "            main_content.append(content[\"text\"])\n",
    "        if cnt >= 10:\n",
    "            break\n",
    "\n",
    "    outputdata = [\n",
    "        {\n",
    "            \"url\": url,\n",
    "            \"questions\": questions,\n",
    "            \"relevant_links\": rl\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    with open('././outputs/output.json', 'w') as f:\n",
    "        json.dump(outputdata, f, indent=4)\n",
    "\n",
    "    print(\"Output Receivedâœ…âœ…\")\n",
    "\n",
    "    pretty_json = json.dumps(outputdata, indent=4, sort_keys=True)\n",
    "    print(pretty_json)\n",
    "\n",
    "    print(\"Output also saved in -> outputs/output.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8df374f-97bc-41a0-a5e2-b67d0f675887",
   "metadata": {},
   "source": [
    "# Website Processing Script\n",
    "\n",
    "This script scrapes a website, generates relevant questions using AI, and saves the output in `outputs/output.json`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c908894-328a-4422-afe7-3868ecb11ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter URL :  https://trumio.ai/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 12:24:19,483 - INFO - Scraping website: https://trumio.ai/\n",
      "2024-08-26 12:24:19,487 - DEBUG - Starting new HTTPS connection (1): trumio.ai:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping website...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 12:24:20,270 - DEBUG - https://trumio.ai:443 \"GET / HTTP/11\" 200 None\n",
      "2024-08-26 12:24:20,368 - DEBUG - Found 106 links on the main page\n",
      "2024-08-26 12:24:20,369 - DEBUG - Processing link: https://trumio.ai\n",
      "2024-08-26 12:24:20,370 - DEBUG - Starting new HTTPS connection (1): trumio.ai:443\n",
      "2024-08-26 12:24:20,890 - DEBUG - https://trumio.ai:443 \"GET / HTTP/11\" 200 None\n",
      "2024-08-26 12:24:21,231 - DEBUG - Processing link: https://trumio.ai/\n",
      "2024-08-26 12:24:21,232 - DEBUG - Starting new HTTPS connection (1): trumio.ai:443\n",
      "2024-08-26 12:24:21,435 - DEBUG - https://trumio.ai:443 \"GET / HTTP/11\" 200 None\n",
      "2024-08-26 12:24:21,486 - DEBUG - Processing link: https://trumio.ai/clients/\n",
      "2024-08-26 12:24:21,487 - DEBUG - Starting new HTTPS connection (1): trumio.ai:443\n",
      "2024-08-26 12:24:23,054 - DEBUG - https://trumio.ai:443 \"GET /clients/ HTTP/11\" 200 None\n",
      "2024-08-26 12:24:23,172 - DEBUG - Processing link: https://trumio.ai/experts/\n",
      "2024-08-26 12:24:23,173 - DEBUG - Starting new HTTPS connection (1): trumio.ai:443\n",
      "2024-08-26 12:24:24,188 - DEBUG - https://trumio.ai:443 \"GET /experts/ HTTP/11\" 200 None\n",
      "2024-08-26 12:24:24,277 - DEBUG - Processing link: https://trumio.ai/student/\n",
      "2024-08-26 12:24:24,278 - DEBUG - Starting new HTTPS connection (1): trumio.ai:443\n",
      "2024-08-26 12:24:25,117 - DEBUG - https://trumio.ai:443 \"GET /student/ HTTP/11\" 200 None\n",
      "2024-08-26 12:24:25,157 - DEBUG - Processing link: https://trumio.ai/clubs/\n",
      "2024-08-26 12:24:25,158 - DEBUG - Starting new HTTPS connection (1): trumio.ai:443\n",
      "2024-08-26 12:24:25,942 - DEBUG - https://trumio.ai:443 \"GET /clubs/ HTTP/11\" 200 None\n",
      "2024-08-26 12:24:26,385 - DEBUG - Processing link: https://trumio.ai/university/\n",
      "2024-08-26 12:24:26,386 - DEBUG - Starting new HTTPS connection (1): trumio.ai:443\n",
      "2024-08-26 12:24:27,274 - DEBUG - https://trumio.ai:443 \"GET /university/ HTTP/11\" 200 None\n",
      "2024-08-26 12:24:27,318 - DEBUG - Processing link: https://trumio.ai/alumni/\n",
      "2024-08-26 12:24:27,319 - DEBUG - Starting new HTTPS connection (1): trumio.ai:443\n",
      "2024-08-26 12:24:30,276 - DEBUG - https://trumio.ai:443 \"GET /alumni/ HTTP/11\" 200 None\n",
      "2024-08-26 12:24:30,587 - DEBUG - Processing link: https://trumio.ai/use-cases\n",
      "2024-08-26 12:24:30,588 - DEBUG - Starting new HTTPS connection (1): trumio.ai:443\n",
      "2024-08-26 12:24:31,197 - DEBUG - https://trumio.ai:443 \"GET /use-cases HTTP/11\" 301 None\n",
      "2024-08-26 12:24:31,919 - DEBUG - https://trumio.ai:443 \"GET /use-cases/ HTTP/11\" 200 None\n",
      "2024-08-26 12:24:32,007 - DEBUG - Processing link: https://trumio.ai/about-us/\n",
      "2024-08-26 12:24:32,008 - DEBUG - Starting new HTTPS connection (1): trumio.ai:443\n",
      "2024-08-26 12:24:32,745 - DEBUG - https://trumio.ai:443 \"GET /about-us/ HTTP/11\" 200 None\n",
      "2024-08-26 12:24:32,781 - DEBUG - Processing link: https://trumio.ai/faq/\n",
      "2024-08-26 12:24:32,781 - DEBUG - Starting new HTTPS connection (1): trumio.ai:443\n",
      "2024-08-26 12:24:35,115 - DEBUG - https://trumio.ai:443 \"GET /faq/ HTTP/11\" 200 None\n",
      "2024-08-26 12:24:35,156 - DEBUG - Processing link: https://trumio.ai/contact-us/\n",
      "2024-08-26 12:24:35,157 - DEBUG - Starting new HTTPS connection (1): trumio.ai:443\n",
      "2024-08-26 12:24:35,942 - DEBUG - https://trumio.ai:443 \"GET /contact-us/ HTTP/11\" 200 None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website scraping completed...âœ… âœ…\n",
      "Generating Questions... Please wait...ðŸ•’ðŸ•’ðŸ•’\n",
      "Output Receivedâœ…âœ…\n",
      "[\n",
      "    {\n",
      "        \"questions\": [\n",
      "            \"What problems does Trumio solve for businesses?\",\n",
      "            \"How does Trumio connect students and experts with clients?\",\n",
      "            \"What is Trumio's AI-powered marketplace for?\",\n",
      "            \"How does Trumio facilitate collaboration between clients and university teams?\",\n",
      "            \"How does Trumio connect clients with university talent?\",\n",
      "            \"What are the key benefits of using Trumio for project development?\",\n",
      "            \"What services does Trumio offer to experts?\",\n",
      "            \"How does Trumio facilitate collaboration between experts and students?\",\n",
      "            \"What are the benefits of using Trumio for students?\",\n",
      "            \"How does Trumio facilitate project collaboration and learning?\"\n",
      "        ],\n",
      "        \"relevant_links\": [\n",
      "            {\n",
      "                \"link\": \"https://trumio.ai\",\n",
      "                \"title\": \"AI-Powered Marketplace for Student & Expert Teams | Trumio\"\n",
      "            },\n",
      "            {\n",
      "                \"link\": \"https://trumio.ai/\",\n",
      "                \"title\": \"AI-Powered Marketplace for Student & Expert Teams | Trumio\"\n",
      "            },\n",
      "            {\n",
      "                \"link\": \"https://trumio.ai/clients/\",\n",
      "                \"title\": \"Hire University Student & Expert Project Teams | Trumio\"\n",
      "            },\n",
      "            {\n",
      "                \"link\": \"https://trumio.ai/experts/\",\n",
      "                \"title\": \"Meet the Expert Team | Trumio\"\n",
      "            },\n",
      "            {\n",
      "                \"link\": \"https://trumio.ai/student/\",\n",
      "                \"title\": \"Student Programs | Trumio\"\n",
      "            }\n",
      "        ],\n",
      "        \"url\": \"https://trumio.ai/\"\n",
      "    }\n",
      "]\n",
      "Output also saved in -> outputs/output.json\n"
     ]
    }
   ],
   "source": [
    "website_url = input(\"Enter URL : \") \n",
    "process_website(website_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9ee5f2-c728-434d-92c3-96376a552994",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "- **`process_website(url)`**: This function handles the end-to-end process of scraping a website, generating questions from the scraped content, and saving the results in JSON format.\n",
    "\n",
    "## Start Instructions\n",
    "\n",
    "### How to Start Locally\n",
    "\n",
    "- Create  Your Virtual Environment\n",
    "- Run the Command in the terminal\n",
    "```\n",
    "python -m venv myenv\n",
    "source myenv/bin/activate\n",
    "pip install -r requirements.txt\n",
    "python que_generation/main.py\n",
    "\n",
    "```\n",
    "\n",
    "### How to Access Through Docker \n",
    "\n",
    "__If You want to use the server code in Docker. Follow the Steps__\n",
    "\n",
    "\n",
    "\n",
    "#### Follow the Steps\n",
    "- Go to the que_generation/main.py\n",
    "- Uncomment the Docker Code Written Down Below and comment the input line\n",
    "- Now Follow the commands\n",
    " \n",
    "```\n",
    "docker build -t your_image_name .\n",
    "\n",
    "docker run -e API_KEY=your_api_key_here -e WEBSITE_URL=\"https://example.com\" your_image_name\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff40e9c9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
